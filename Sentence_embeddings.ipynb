{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stance detection\n",
    "\n",
    "After reviewing the latest literature on the SemEval2016, I think it's a good starting point to formulate the problem into a text classification with sentence-pair inputs (keeping it simple!). However, I suggest using pre-trained language models to generate meaningful sentence embeddings, rather than training the model from scratch on the available data. \n",
    "\n",
    "The language models used are:<br>\n",
    "1- Google's BERT model [1]. Bidirectional Transformers for Language Understanding [2] is arguably the best pre-trained language model available; capable of achieving state-of-the-art results in various NLP tasks. \n",
    "\n",
    "2- Flair embeddings [3,4]. Contextual String Embeddings for Sequence Labeling is currently the state-of-the-art [4] system in Named Entity Recognition task, and the only system outperforming Google's BERT model in this application. \n",
    "\n",
    "Both models are expensive to use (especially on my potato laptop), however, using them improve my chances of achieving better results. I also wanted an excuse to play with them :)\n",
    "\n",
    "The suggested architecture looks like this..\n",
    "\n",
    "\n",
    "\n",
    "I experiment with three types of embeddings;<br>\n",
    "1- Flair's Document Pool Embeddings<br>\n",
    "2- Flair's Document LSTM Embeddings<br>\n",
    "3- Google's BERT Embeddings<br>\n",
    "\n",
    "At the end of this code, I suggest further improvements that can help improve the obtained results.\n",
    "\n",
    "Requirements to run this code:\n",
    "- python 3.6\n",
    "- bert\n",
    "- flair\n",
    "\n",
    "[1] https://github.com/google-research/bert<br>\n",
    "[2] https://arxiv.org/abs/1810.04805<br>\n",
    "[3] https://github.com/zalandoresearch/flair<br>\n",
    "[4] https://drive.google.com/file/d/17yVpFA7MmXaQFTe-HDpZuqw9fJlmzg56/view<br>\n",
    "[5] https://github.com/zalandoresearch/flair#comparison-with-state-of-the-art<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initlaization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading/Inspecting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data has 2914 instances\n",
      "Hillary Clinton                     689\n",
      "Feminist Movement                   664\n",
      "Legalization of Abortion            653\n",
      "Atheism                             513\n",
      "Climate Change is a Real Concern    395\n",
      "Name: Target, dtype: int64 \n",
      "\n",
      "Test data has 1249 instances\n",
      "Hillary Clinton                     295\n",
      "Feminist Movement                   285\n",
      "Legalization of Abortion            280\n",
      "Atheism                             220\n",
      "Climate Change is a Real Concern    169\n",
      "Name: Target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'Dataset/'\n",
    "\n",
    "#=------------------------------------------------=#\n",
    "## Training data\n",
    "Training_data = []\n",
    "\n",
    "with open(dataset_path + 'SemEval2016-Task6-subtaskA-traindata-gold.csv', 'r',  encoding=\"iso-8859-1\") as fin:\n",
    "    reader = csv.reader(fin, quotechar='\"')\n",
    "    columns = next(reader)\n",
    "    for line in reader:\n",
    "        Training_data.append(line)\n",
    "        \n",
    "train_df = pd.DataFrame(Training_data, columns=columns)\n",
    "classes = list( set(train_df['Stance']) )\n",
    "\n",
    "print('Training data has %d instances' %(len(Training_data,)))\n",
    "print(train_df['Target'].value_counts(), '\\n')\n",
    "\n",
    "#=------------------------------------------------=#\n",
    "## Test data\n",
    "Test_data = []\n",
    "\n",
    "with open(dataset_path + 'SemEval2016-Task6-subtaskA-testdata-gold.txt', 'r',  encoding=\"iso-8859-1\") as fin:\n",
    "    reader = csv.reader(fin, delimiter='\\t')\n",
    "    columns = next(reader)\n",
    "    for line in reader:\n",
    "        Test_data.append(line)\n",
    "\n",
    "test_df = pd.DataFrame(Test_data, columns=columns)\n",
    "\n",
    "print('Test data has %d instances' %(len(Test_data,)))\n",
    "print(test_df['Target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  *Training has ( 2619 ) instances.\n",
      "  *Validation has ( 295 ) instances.\n",
      "  *Test has ( 1249 ) instances.\n"
     ]
    }
   ],
   "source": [
    "def _check_dir(_dir):\n",
    "    output_dir = Path(_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "\n",
    "flair_dir = 'Flair/'\n",
    "_check_dir(flair_dir)\n",
    "\n",
    "path_save_data = 'Flair/data'\n",
    "_check_dir(path_save_data)\n",
    "\n",
    "Targets = train_df['Target'].values\n",
    "Tweets = train_df['Tweet'].values\n",
    "Stances = train_df['Stance'].values\n",
    "\n",
    "data = [[stance, target, tweet] for stance, target, tweet in zip(Stances, Targets, Tweets)]\n",
    "\n",
    "random.shuffle(data)    # shuffling the data is always good to preven overfitting\n",
    "\n",
    "# dividing the data into trainig (90%), validation (10%).\n",
    "split_ = int(0.1 * len(data))\n",
    "TRAIN_DATA, VAL_DATA = data[:9*split_], data[9*split_:]\n",
    "print('  *Training has (',len(TRAIN_DATA),') instances.')\n",
    "print('  *Validation has (',len(VAL_DATA),') instances.')\n",
    "\n",
    "Targets = test_df['Target'].values\n",
    "Tweets = test_df['Tweet'].values\n",
    "Stances = test_df['Stance'].values\n",
    "\n",
    "TEST_DATA = [[stance, target, tweet] for stance, target, tweet in zip(Stances, Targets, Tweets)]\n",
    "\n",
    "print('  *Test has (',len(TEST_DATA),') instances.')\n",
    "\n",
    "\n",
    "for name, data in zip(['train','val','test'],[TRAIN_DATA, VAL_DATA, TEST_DATA]):\n",
    "    pickle.dump(data, open(path_save_data+'/'+name+'.p','wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, CharLMEmbeddings, DocumentPoolEmbeddings, DocumentLSTMEmbeddings\n",
    "from flair.data import Sentence, TaggedCorpus, Token\n",
    "\n",
    "# initialize the word embeddings\n",
    "# the -fast embeddings are CPU friendly\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "charlm_embedding_forward = CharLMEmbeddings('news-forward-fast')\n",
    "charlm_embedding_backward = CharLMEmbeddings('news-backward-fast')\n",
    "\n",
    "# initialize the document embeddings\n",
    "\n",
    "# Embedding(1)\n",
    "# glove = 100\n",
    "# charlm_embedding_backward = 1024\n",
    "# charlm_embedding_forward = 1024\n",
    "document_embeddings1 = DocumentPoolEmbeddings([glove_embedding,\n",
    "                                              charlm_embedding_backward,\n",
    "                                              charlm_embedding_forward])\n",
    "\n",
    "\n",
    "# Embedding(2)\n",
    "# a total of 128 vector generated by an LSTM\n",
    "document_embeddings2 = DocumentLSTMEmbeddings([glove_embedding,\n",
    "                                              charlm_embedding_backward,\n",
    "                                              charlm_embedding_forward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -processed:0 examples\n",
      "  -processed:0 examples\n",
      "  -processed:0 examples\n",
      "tensor([[ 2.7323e-01,  6.0706e-01,  1.7386e-01,  ..., -2.0105e-08,\n",
      "          8.3940e-04,  1.6731e-02],\n",
      "        [ 2.7323e-01,  6.0706e-01,  1.7386e-01,  ..., -3.9551e-08,\n",
      "         -3.6231e-05,  2.1956e-02]])\n"
     ]
    }
   ],
   "source": [
    "path_save_embd = 'Flair/embeddings'\n",
    "_check_dir(path_save_embd)\n",
    "\n",
    "TRAIN_DATA = pickle.load(open(path_save_data+'/train.p','rb'))\n",
    "VAL_DATA = pickle.load(open(path_save_data+'/val.p','rb'))\n",
    "TEST_DATA = pickle.load(open(path_save_data+'/test.p','rb'))\n",
    "    \n",
    "def _get_embeddings(length, data):\n",
    "    Y = torch.zeros([length,3])\n",
    "    X1 = torch.zeros([length,2148*2]) # Pool Embeddings\n",
    "    X2 = torch.zeros([length,256])  # LSTM Embeddings\n",
    "    \n",
    "    X_target = {} ## store the target embeddings to prevent recalucalting them each time\n",
    "\n",
    "    for counter, data in enumerate(data[:length]):\n",
    "        stance, target, tweet = data\n",
    "        if np.mod(counter,100)==0:\n",
    "            print('  -processed:%d examples' %(counter))\n",
    "\n",
    "        Y[counter,classes.index(stance)] = 1\n",
    "\n",
    "        # create an example sentence\n",
    "        if target not in X_target:\n",
    "            sentence1_1 = Sentence(target)\n",
    "            sentence1_2 = Sentence(target)\n",
    "            \n",
    "            document_embeddings1.embed(sentence1_1)\n",
    "            document_embeddings2.embed(sentence1_2)\n",
    "            \n",
    "            embd_T1 = sentence1_1.get_embedding()[0]\n",
    "            embd_T2 = sentence1_2.get_embedding()[0]\n",
    "            \n",
    "            X_target[target] = [embd_T1, embd_T2]\n",
    "        else:\n",
    "            embd_T1, embd_T2 = X_target[target]\n",
    "        \n",
    "        \n",
    "        # create an example sentence\n",
    "        # embed the sentence with our document embedding\n",
    "        sentence2_1 = Sentence(tweet)\n",
    "        sentence2_2 = Sentence(tweet)\n",
    "        \n",
    "        document_embeddings1.embed(sentence2_1)\n",
    "        document_embeddings2.embed(sentence2_2)\n",
    "        \n",
    "        embd1 = sentence2_1.get_embedding()[0]\n",
    "        embd2 = sentence2_2.get_embedding()[0]\n",
    "        \n",
    "        X1[counter,:] = torch.cat((embd_T1, embd1), 0).data\n",
    "        X2[counter,:] = torch.cat((embd_T2, embd2), 0).data\n",
    "    return [X1,X2,Y]\n",
    "\n",
    "TRAIN_EMBD = _get_embeddings(len(TRAIN_DATA), TRAIN_DATA)\n",
    "VAL_EMBD = _get_embeddings(len(VAL_DATA), VAL_DATA)\n",
    "TEST_EMBD = _get_embeddings(len(TEST_DATA), TEST_DATA)\n",
    "\n",
    "pickle.dump(TRAIN_EMBD, open(path_save_embd+'/train_embd.p', 'wb'))\n",
    "pickle.dump(VAL_EMBD, open(path_save_embd+'/val_embd.p', 'wb'))\n",
    "pickle.dump(TEST_EMBD, open(path_save_embd+'/test_embd.p', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training NN models with the extracted embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4296, out_features=600, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=600, out_features=200, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=200, out_features=40, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=40, out_features=3, bias=True)\n",
      "  (7): Softmax()\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=256, out_features=200, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=100, out_features=40, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=40, out_features=3, bias=True)\n",
      "  (7): Softmax()\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=256, out_features=200, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=100, out_features=40, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=40, out_features=3, bias=True)\n",
      "  (7): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# This function is used to create new models\n",
    "# I create a new model for every embedding \n",
    "def _create_model(n_in, n_h1, n_h2, n_h3, n_out, lr, wd):\n",
    "    model = nn.Sequential(nn.Linear(n_in, n_h1),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(n_h1, n_h2),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(n_h2, n_h3),\n",
    "                         nn.ReLU(),\n",
    "                         nn.Linear(n_h3, n_out),\n",
    "                         nn.Softmax())\n",
    "    return model\n",
    "\n",
    "# This function is used to train a given model\n",
    "def train(model, data, criterion, optimizer, epoch, epochs):\n",
    "    # measure time\n",
    "    start = time.time()\n",
    "    \n",
    "    # extract training data\n",
    "    x,y = data\n",
    "    \n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Forward Propagation\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print training loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # perform a backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Train Epoch: [%d/%d] Losses: [%.6f] Time: %.3f sec.' %(epoch, epochs, loss.item(), time.time() - start))\n",
    "\n",
    "    # clear memroy\n",
    "    gc.collect()\n",
    "\n",
    "# Test models given validation or test data\n",
    "def test(model, data, criterion, epoch, epochs, flag):\n",
    "    # measure time\n",
    "    start = time.time()\n",
    "\n",
    "    # extract training data\n",
    "    x,y = data\n",
    "    \n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Forward Propagation\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print validation loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    # Computer other measure\n",
    "    y_true = [int(torch.max(i, 0)[1].item()) for i in y]\n",
    "    y_pred = [int(torch.max(i, 0)[1].item()) for i in y_pred]\n",
    "    \n",
    "    P = precision_score(y_true, y_pred, average='micro') \n",
    "    R = recall_score(y_true, y_pred, average='micro')\n",
    "    A = accuracy_score(y_true, y_pred)\n",
    "    F1 = f1_score(y_true, y_pred, average='micro')\n",
    "    T = time.time() - start\n",
    "    \n",
    "    # print the validation updated measures\n",
    "    print('Validation_: [%d/%d] Losses: [%.3f] Precision: [%.3f]'\n",
    "          ' Recall: [%.3f] Accuracy [%.3f] f1-score: [%.3f] Time'\n",
    "          ': %.2f sec.' %(epoch, epochs, loss.item(), P, R, A, F1, T))\n",
    "\n",
    "    # this is just to make it clear when we print\n",
    "    if flag:\n",
    "        print('  =------=  ')\n",
    "\n",
    "    # clear memroy\n",
    "    gc.collect()\n",
    "\n",
    "    \n",
    "#---------------------------------------------------------#\n",
    "# Creating three models;\n",
    "#   1- model1 for Flair's DocumentPoolEmbeddings\n",
    "#   2- model2 for Flair's DocumentLSTMEmbeddings\n",
    "#   3- model3 for Google's BERT embeddings\n",
    "model1 = _create_model(4296, 600, 200, 40, 3, 1e-1, 1e-3)\n",
    "model2 = _create_model(256, 200, 100, 40, 3, 1e-1, 1e-3)\n",
    "model3 = _create_model(256, 200, 100, 40, 3, 1e-1, 1e-3)\n",
    "            \n",
    "print(model1)\n",
    "print(model2)\n",
    "print(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: [1/60] Losses: [1777.541992] Time: 0.213 sec.\n",
      "Validation_: [1/60] Losses: [185.223] Precision: [0.505] Recall: [0.505] Accuracy [0.505] f1-score: [0.505] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [2/60] Losses: [1730.277466] Time: 0.258 sec.\n",
      "Validation_: [2/60] Losses: [284.687] Precision: [0.275] Recall: [0.275] Accuracy [0.275] f1-score: [0.275] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [3/60] Losses: [2466.174072] Time: 0.258 sec.\n",
      "Validation_: [3/60] Losses: [277.575] Precision: [0.220] Recall: [0.220] Accuracy [0.220] f1-score: [0.220] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [4/60] Losses: [2367.302979] Time: 0.288 sec.\n",
      "Validation_: [4/60] Losses: [209.134] Precision: [0.220] Recall: [0.220] Accuracy [0.220] f1-score: [0.220] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [5/60] Losses: [1836.895630] Time: 0.217 sec.\n",
      "Validation_: [5/60] Losses: [199.586] Precision: [0.254] Recall: [0.254] Accuracy [0.254] f1-score: [0.254] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [6/60] Losses: [1763.495361] Time: 0.218 sec.\n",
      "Validation_: [6/60] Losses: [194.792] Precision: [0.502] Recall: [0.502] Accuracy [0.502] f1-score: [0.502] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [7/60] Losses: [1730.144531] Time: 0.257 sec.\n",
      "Validation_: [7/60] Losses: [190.100] Precision: [0.505] Recall: [0.505] Accuracy [0.505] f1-score: [0.505] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [8/60] Losses: [1702.053101] Time: 0.264 sec.\n",
      "Validation_: [8/60] Losses: [186.361] Precision: [0.505] Recall: [0.505] Accuracy [0.505] f1-score: [0.505] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [9/60] Losses: [1674.127808] Time: 0.288 sec.\n",
      "Validation_: [9/60] Losses: [182.708] Precision: [0.505] Recall: [0.505] Accuracy [0.505] f1-score: [0.505] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [10/60] Losses: [1644.446899] Time: 0.264 sec.\n",
      "Validation_: [10/60] Losses: [177.351] Precision: [0.522] Recall: [0.522] Accuracy [0.522] f1-score: [0.522] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [11/60] Losses: [1606.503052] Time: 0.263 sec.\n",
      "Validation_: [11/60] Losses: [175.244] Precision: [0.569] Recall: [0.569] Accuracy [0.569] f1-score: [0.569] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [12/60] Losses: [1578.818115] Time: 0.219 sec.\n",
      "Validation_: [12/60] Losses: [172.999] Precision: [0.569] Recall: [0.569] Accuracy [0.569] f1-score: [0.569] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [13/60] Losses: [1610.839478] Time: 0.260 sec.\n",
      "Validation_: [13/60] Losses: [182.571] Precision: [0.522] Recall: [0.522] Accuracy [0.522] f1-score: [0.522] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [14/60] Losses: [1620.773438] Time: 0.215 sec.\n",
      "Validation_: [14/60] Losses: [171.939] Precision: [0.569] Recall: [0.569] Accuracy [0.569] f1-score: [0.569] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [15/60] Losses: [1564.645874] Time: 0.220 sec.\n",
      "Validation_: [15/60] Losses: [169.720] Precision: [0.569] Recall: [0.569] Accuracy [0.569] f1-score: [0.569] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [16/60] Losses: [1572.169800] Time: 0.211 sec.\n",
      "Validation_: [16/60] Losses: [168.689] Precision: [0.569] Recall: [0.569] Accuracy [0.569] f1-score: [0.569] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [17/60] Losses: [1536.943970] Time: 0.212 sec.\n",
      "Validation_: [17/60] Losses: [177.390] Precision: [0.525] Recall: [0.525] Accuracy [0.525] f1-score: [0.525] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [18/60] Losses: [1573.066895] Time: 0.210 sec.\n",
      "Validation_: [18/60] Losses: [168.551] Precision: [0.569] Recall: [0.569] Accuracy [0.569] f1-score: [0.569] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [19/60] Losses: [1527.613281] Time: 0.264 sec.\n",
      "Validation_: [19/60] Losses: [167.214] Precision: [0.569] Recall: [0.569] Accuracy [0.569] f1-score: [0.569] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [20/60] Losses: [1541.042358] Time: 0.215 sec.\n",
      "Validation_: [20/60] Losses: [166.915] Precision: [0.569] Recall: [0.569] Accuracy [0.569] f1-score: [0.569] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [21/60] Losses: [1535.094482] Time: 0.266 sec.\n",
      "Validation_: [21/60] Losses: [167.176] Precision: [0.569] Recall: [0.569] Accuracy [0.569] f1-score: [0.569] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [22/60] Losses: [1519.079346] Time: 0.342 sec.\n",
      "Validation_: [22/60] Losses: [169.200] Precision: [0.563] Recall: [0.563] Accuracy [0.563] f1-score: [0.563] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [23/60] Losses: [1519.772949] Time: 0.327 sec.\n",
      "Validation_: [23/60] Losses: [167.508] Precision: [0.566] Recall: [0.566] Accuracy [0.566] f1-score: [0.566] Time: 0.04 sec.\n",
      "  =------=  \n",
      "Train Epoch: [24/60] Losses: [1505.900513] Time: 0.440 sec.\n",
      "Validation_: [24/60] Losses: [166.001] Precision: [0.569] Recall: [0.569] Accuracy [0.569] f1-score: [0.569] Time: 0.04 sec.\n",
      "  =------=  \n",
      "Train Epoch: [25/60] Losses: [1501.133179] Time: 0.368 sec.\n",
      "Validation_: [25/60] Losses: [165.619] Precision: [0.569] Recall: [0.569] Accuracy [0.569] f1-score: [0.569] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [26/60] Losses: [1495.979614] Time: 0.358 sec.\n",
      "Validation_: [26/60] Losses: [166.339] Precision: [0.569] Recall: [0.569] Accuracy [0.569] f1-score: [0.569] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [27/60] Losses: [1484.998169] Time: 0.338 sec.\n",
      "Validation_: [27/60] Losses: [167.207] Precision: [0.569] Recall: [0.569] Accuracy [0.569] f1-score: [0.569] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [28/60] Losses: [1483.118164] Time: 0.478 sec.\n",
      "Validation_: [28/60] Losses: [163.653] Precision: [0.569] Recall: [0.569] Accuracy [0.569] f1-score: [0.569] Time: 0.04 sec.\n",
      "  =------=  \n",
      "Train Epoch: [29/60] Losses: [1463.520264] Time: 0.410 sec.\n",
      "Validation_: [29/60] Losses: [162.564] Precision: [0.573] Recall: [0.573] Accuracy [0.573] f1-score: [0.573] Time: 0.05 sec.\n",
      "  =------=  \n",
      "Train Epoch: [30/60] Losses: [1460.120239] Time: 0.404 sec.\n",
      "Validation_: [30/60] Losses: [164.091] Precision: [0.580] Recall: [0.580] Accuracy [0.580] f1-score: [0.580] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [31/60] Losses: [1452.984375] Time: 0.334 sec.\n",
      "Validation_: [31/60] Losses: [160.257] Precision: [0.583] Recall: [0.583] Accuracy [0.583] f1-score: [0.583] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [32/60] Losses: [1433.517090] Time: 0.339 sec.\n",
      "Validation_: [32/60] Losses: [159.496] Precision: [0.583] Recall: [0.583] Accuracy [0.583] f1-score: [0.583] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [33/60] Losses: [1425.869995] Time: 0.443 sec.\n",
      "Validation_: [33/60] Losses: [165.136] Precision: [0.566] Recall: [0.566] Accuracy [0.566] f1-score: [0.566] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [34/60] Losses: [1433.402710] Time: 0.356 sec.\n",
      "Validation_: [34/60] Losses: [164.175] Precision: [0.569] Recall: [0.569] Accuracy [0.569] f1-score: [0.569] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [35/60] Losses: [1469.150391] Time: 0.332 sec.\n",
      "Validation_: [35/60] Losses: [167.642] Precision: [0.539] Recall: [0.539] Accuracy [0.539] f1-score: [0.539] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [36/60] Losses: [1427.724243] Time: 0.335 sec.\n",
      "Validation_: [36/60] Losses: [159.371] Precision: [0.603] Recall: [0.603] Accuracy [0.603] f1-score: [0.603] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [37/60] Losses: [1381.836548] Time: 0.334 sec.\n",
      "Validation_: [37/60] Losses: [159.087] Precision: [0.607] Recall: [0.607] Accuracy [0.607] f1-score: [0.607] Time: 0.05 sec.\n",
      "  =------=  \n",
      "Train Epoch: [38/60] Losses: [1415.113647] Time: 0.329 sec.\n",
      "Validation_: [38/60] Losses: [161.795] Precision: [0.580] Recall: [0.580] Accuracy [0.580] f1-score: [0.580] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [39/60] Losses: [1385.659180] Time: 0.327 sec.\n",
      "Validation_: [39/60] Losses: [157.468] Precision: [0.586] Recall: [0.586] Accuracy [0.586] f1-score: [0.586] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [40/60] Losses: [1354.049072] Time: 0.347 sec.\n",
      "Validation_: [40/60] Losses: [156.150] Precision: [0.607] Recall: [0.607] Accuracy [0.607] f1-score: [0.607] Time: 0.04 sec.\n",
      "  =------=  \n",
      "Train Epoch: [41/60] Losses: [1369.308594] Time: 0.336 sec.\n",
      "Validation_: [41/60] Losses: [158.035] Precision: [0.600] Recall: [0.600] Accuracy [0.600] f1-score: [0.600] Time: 0.04 sec.\n",
      "  =------=  \n",
      "Train Epoch: [42/60] Losses: [1340.744385] Time: 0.345 sec.\n",
      "Validation_: [42/60] Losses: [155.927] Precision: [0.590] Recall: [0.590] Accuracy [0.590] f1-score: [0.590] Time: 0.03 sec.\n",
      "  =------=  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: [43/60] Losses: [1321.013428] Time: 0.372 sec.\n",
      "Validation_: [43/60] Losses: [153.314] Precision: [0.603] Recall: [0.603] Accuracy [0.603] f1-score: [0.603] Time: 0.04 sec.\n",
      "  =------=  \n",
      "Train Epoch: [44/60] Losses: [1332.932861] Time: 0.453 sec.\n",
      "Validation_: [44/60] Losses: [153.206] Precision: [0.610] Recall: [0.610] Accuracy [0.610] f1-score: [0.610] Time: 0.04 sec.\n",
      "  =------=  \n",
      "Train Epoch: [45/60] Losses: [1305.735962] Time: 0.366 sec.\n",
      "Validation_: [45/60] Losses: [152.279] Precision: [0.614] Recall: [0.614] Accuracy [0.614] f1-score: [0.614] Time: 0.04 sec.\n",
      "  =------=  \n",
      "Train Epoch: [46/60] Losses: [1285.309326] Time: 0.426 sec.\n",
      "Validation_: [46/60] Losses: [150.914] Precision: [0.607] Recall: [0.607] Accuracy [0.607] f1-score: [0.607] Time: 0.04 sec.\n",
      "  =------=  \n",
      "Train Epoch: [47/60] Losses: [1294.194824] Time: 0.316 sec.\n",
      "Validation_: [47/60] Losses: [155.082] Precision: [0.603] Recall: [0.603] Accuracy [0.603] f1-score: [0.603] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [48/60] Losses: [1287.824219] Time: 0.356 sec.\n",
      "Validation_: [48/60] Losses: [149.076] Precision: [0.603] Recall: [0.603] Accuracy [0.603] f1-score: [0.603] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [49/60] Losses: [1260.099487] Time: 0.423 sec.\n",
      "Validation_: [49/60] Losses: [148.598] Precision: [0.610] Recall: [0.610] Accuracy [0.610] f1-score: [0.610] Time: 0.04 sec.\n",
      "  =------=  \n",
      "Train Epoch: [50/60] Losses: [1234.392700] Time: 0.313 sec.\n",
      "Validation_: [50/60] Losses: [150.875] Precision: [0.614] Recall: [0.614] Accuracy [0.614] f1-score: [0.614] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [51/60] Losses: [1233.703979] Time: 0.314 sec.\n",
      "Validation_: [51/60] Losses: [147.947] Precision: [0.610] Recall: [0.610] Accuracy [0.610] f1-score: [0.610] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [52/60] Losses: [1253.015747] Time: 0.319 sec.\n",
      "Validation_: [52/60] Losses: [153.224] Precision: [0.593] Recall: [0.593] Accuracy [0.593] f1-score: [0.593] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [53/60] Losses: [1248.691528] Time: 0.505 sec.\n",
      "Validation_: [53/60] Losses: [147.807] Precision: [0.620] Recall: [0.620] Accuracy [0.620] f1-score: [0.620] Time: 0.07 sec.\n",
      "  =------=  \n",
      "Train Epoch: [54/60] Losses: [1217.089478] Time: 0.410 sec.\n",
      "Validation_: [54/60] Losses: [151.915] Precision: [0.593] Recall: [0.593] Accuracy [0.593] f1-score: [0.593] Time: 0.04 sec.\n",
      "  =------=  \n",
      "Train Epoch: [55/60] Losses: [1203.416870] Time: 0.334 sec.\n",
      "Validation_: [55/60] Losses: [148.955] Precision: [0.614] Recall: [0.614] Accuracy [0.614] f1-score: [0.614] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [56/60] Losses: [1170.269531] Time: 0.330 sec.\n",
      "Validation_: [56/60] Losses: [145.287] Precision: [0.617] Recall: [0.617] Accuracy [0.617] f1-score: [0.617] Time: 0.05 sec.\n",
      "  =------=  \n",
      "Train Epoch: [57/60] Losses: [1180.687500] Time: 0.426 sec.\n",
      "Validation_: [57/60] Losses: [155.766] Precision: [0.586] Recall: [0.586] Accuracy [0.586] f1-score: [0.586] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [58/60] Losses: [1223.837280] Time: 0.338 sec.\n",
      "Validation_: [58/60] Losses: [153.406] Precision: [0.627] Recall: [0.627] Accuracy [0.627] f1-score: [0.627] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [59/60] Losses: [1252.520752] Time: 0.476 sec.\n",
      "Validation_: [59/60] Losses: [153.669] Precision: [0.590] Recall: [0.590] Accuracy [0.590] f1-score: [0.590] Time: 0.06 sec.\n",
      "  =------=  \n",
      "Train Epoch: [60/60] Losses: [1148.787842] Time: 0.472 sec.\n",
      "Validation_: [60/60] Losses: [151.906] Precision: [0.607] Recall: [0.607] Accuracy [0.607] f1-score: [0.607] Time: 0.04 sec.\n",
      "  =------=  \n",
      "Train Epoch: [1/60] Losses: [1727.420288] Time: 0.111 sec.\n",
      "Validation_: [1/60] Losses: [185.690] Precision: [0.505] Recall: [0.505] Accuracy [0.505] f1-score: [0.505] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [2/60] Losses: [1635.448730] Time: 0.024 sec.\n",
      "Validation_: [2/60] Losses: [199.312] Precision: [0.505] Recall: [0.505] Accuracy [0.505] f1-score: [0.505] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [3/60] Losses: [1867.052368] Time: 0.026 sec.\n",
      "Validation_: [3/60] Losses: [187.724] Precision: [0.505] Recall: [0.505] Accuracy [0.505] f1-score: [0.505] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [4/60] Losses: [1592.561157] Time: 0.020 sec.\n",
      "Validation_: [4/60] Losses: [193.860] Precision: [0.475] Recall: [0.475] Accuracy [0.475] f1-score: [0.475] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [5/60] Losses: [1648.579834] Time: 0.022 sec.\n",
      "Validation_: [5/60] Losses: [194.040] Precision: [0.458] Recall: [0.458] Accuracy [0.458] f1-score: [0.458] Time: 0.03 sec.\n",
      "  =------=  \n",
      "Train Epoch: [6/60] Losses: [1645.027710] Time: 0.020 sec.\n",
      "Validation_: [6/60] Losses: [193.356] Precision: [0.447] Recall: [0.447] Accuracy [0.447] f1-score: [0.447] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [7/60] Losses: [1623.704102] Time: 0.019 sec.\n",
      "Validation_: [7/60] Losses: [192.093] Precision: [0.468] Recall: [0.468] Accuracy [0.468] f1-score: [0.468] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [8/60] Losses: [1599.651001] Time: 0.020 sec.\n",
      "Validation_: [8/60] Losses: [189.931] Precision: [0.505] Recall: [0.505] Accuracy [0.505] f1-score: [0.505] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [9/60] Losses: [1577.893677] Time: 0.019 sec.\n",
      "Validation_: [9/60] Losses: [186.929] Precision: [0.512] Recall: [0.512] Accuracy [0.512] f1-score: [0.512] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [10/60] Losses: [1549.566040] Time: 0.018 sec.\n",
      "Validation_: [10/60] Losses: [185.128] Precision: [0.505] Recall: [0.505] Accuracy [0.505] f1-score: [0.505] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [11/60] Losses: [1540.318604] Time: 0.019 sec.\n",
      "Validation_: [11/60] Losses: [184.668] Precision: [0.505] Recall: [0.505] Accuracy [0.505] f1-score: [0.505] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [12/60] Losses: [1553.463989] Time: 0.017 sec.\n",
      "Validation_: [12/60] Losses: [184.728] Precision: [0.505] Recall: [0.505] Accuracy [0.505] f1-score: [0.505] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [13/60] Losses: [1544.924316] Time: 0.018 sec.\n",
      "Validation_: [13/60] Losses: [185.233] Precision: [0.505] Recall: [0.505] Accuracy [0.505] f1-score: [0.505] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [14/60] Losses: [1520.997925] Time: 0.019 sec.\n",
      "Validation_: [14/60] Losses: [185.978] Precision: [0.515] Recall: [0.515] Accuracy [0.515] f1-score: [0.515] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [15/60] Losses: [1509.939331] Time: 0.017 sec.\n",
      "Validation_: [15/60] Losses: [186.683] Precision: [0.542] Recall: [0.542] Accuracy [0.542] f1-score: [0.542] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [16/60] Losses: [1512.457397] Time: 0.018 sec.\n",
      "Validation_: [16/60] Losses: [187.199] Precision: [0.522] Recall: [0.522] Accuracy [0.522] f1-score: [0.522] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [17/60] Losses: [1512.719360] Time: 0.019 sec.\n",
      "Validation_: [17/60] Losses: [187.329] Precision: [0.512] Recall: [0.512] Accuracy [0.512] f1-score: [0.512] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [18/60] Losses: [1506.163574] Time: 0.026 sec.\n",
      "Validation_: [18/60] Losses: [187.210] Precision: [0.495] Recall: [0.495] Accuracy [0.495] f1-score: [0.495] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [19/60] Losses: [1496.213623] Time: 0.026 sec.\n",
      "Validation_: [19/60] Losses: [187.169] Precision: [0.502] Recall: [0.502] Accuracy [0.502] f1-score: [0.502] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [20/60] Losses: [1489.204712] Time: 0.025 sec.\n",
      "Validation_: [20/60] Losses: [187.472] Precision: [0.505] Recall: [0.505] Accuracy [0.505] f1-score: [0.505] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [21/60] Losses: [1487.727539] Time: 0.022 sec.\n",
      "Validation_: [21/60] Losses: [188.362] Precision: [0.495] Recall: [0.495] Accuracy [0.495] f1-score: [0.495] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [22/60] Losses: [1483.124878] Time: 0.030 sec.\n",
      "Validation_: [22/60] Losses: [190.076] Precision: [0.475] Recall: [0.475] Accuracy [0.475] f1-score: [0.475] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [23/60] Losses: [1471.960083] Time: 0.026 sec.\n",
      "Validation_: [23/60] Losses: [192.436] Precision: [0.485] Recall: [0.485] Accuracy [0.485] f1-score: [0.485] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [24/60] Losses: [1463.059692] Time: 0.022 sec.\n",
      "Validation_: [24/60] Losses: [194.375] Precision: [0.454] Recall: [0.454] Accuracy [0.454] f1-score: [0.454] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [25/60] Losses: [1457.262695] Time: 0.022 sec.\n",
      "Validation_: [25/60] Losses: [196.284] Precision: [0.447] Recall: [0.447] Accuracy [0.447] f1-score: [0.447] Time: 0.01 sec.\n",
      "  =------=  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: [26/60] Losses: [1446.453613] Time: 0.021 sec.\n",
      "Validation_: [26/60] Losses: [197.272] Precision: [0.403] Recall: [0.403] Accuracy [0.403] f1-score: [0.403] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [27/60] Losses: [1430.726807] Time: 0.020 sec.\n",
      "Validation_: [27/60] Losses: [199.103] Precision: [0.386] Recall: [0.386] Accuracy [0.386] f1-score: [0.386] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [28/60] Losses: [1419.278687] Time: 0.017 sec.\n",
      "Validation_: [28/60] Losses: [201.009] Precision: [0.369] Recall: [0.369] Accuracy [0.369] f1-score: [0.369] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [29/60] Losses: [1400.671753] Time: 0.029 sec.\n",
      "Validation_: [29/60] Losses: [205.217] Precision: [0.353] Recall: [0.353] Accuracy [0.353] f1-score: [0.353] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [30/60] Losses: [1381.050171] Time: 0.023 sec.\n",
      "Validation_: [30/60] Losses: [209.397] Precision: [0.353] Recall: [0.353] Accuracy [0.353] f1-score: [0.353] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [31/60] Losses: [1362.845215] Time: 0.021 sec.\n",
      "Validation_: [31/60] Losses: [207.859] Precision: [0.349] Recall: [0.349] Accuracy [0.349] f1-score: [0.349] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [32/60] Losses: [1348.982788] Time: 0.024 sec.\n",
      "Validation_: [32/60] Losses: [212.608] Precision: [0.339] Recall: [0.339] Accuracy [0.339] f1-score: [0.339] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [33/60] Losses: [1332.447388] Time: 0.045 sec.\n",
      "Validation_: [33/60] Losses: [214.248] Precision: [0.322] Recall: [0.322] Accuracy [0.322] f1-score: [0.322] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [34/60] Losses: [1295.150269] Time: 0.031 sec.\n",
      "Validation_: [34/60] Losses: [218.187] Precision: [0.312] Recall: [0.312] Accuracy [0.312] f1-score: [0.312] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [35/60] Losses: [1289.611084] Time: 0.028 sec.\n",
      "Validation_: [35/60] Losses: [223.945] Precision: [0.281] Recall: [0.281] Accuracy [0.281] f1-score: [0.281] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [36/60] Losses: [1261.854736] Time: 0.020 sec.\n",
      "Validation_: [36/60] Losses: [219.030] Precision: [0.302] Recall: [0.302] Accuracy [0.302] f1-score: [0.302] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [37/60] Losses: [1236.502319] Time: 0.023 sec.\n",
      "Validation_: [37/60] Losses: [225.705] Precision: [0.288] Recall: [0.288] Accuracy [0.288] f1-score: [0.288] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [38/60] Losses: [1214.840576] Time: 0.021 sec.\n",
      "Validation_: [38/60] Losses: [228.963] Precision: [0.285] Recall: [0.285] Accuracy [0.285] f1-score: [0.285] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [39/60] Losses: [1192.741089] Time: 0.033 sec.\n",
      "Validation_: [39/60] Losses: [238.668] Precision: [0.281] Recall: [0.281] Accuracy [0.281] f1-score: [0.281] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [40/60] Losses: [1172.319824] Time: 0.048 sec.\n",
      "Validation_: [40/60] Losses: [231.706] Precision: [0.298] Recall: [0.298] Accuracy [0.298] f1-score: [0.298] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [41/60] Losses: [1146.291016] Time: 0.021 sec.\n",
      "Validation_: [41/60] Losses: [236.209] Precision: [0.292] Recall: [0.292] Accuracy [0.292] f1-score: [0.292] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [42/60] Losses: [1117.929932] Time: 0.033 sec.\n",
      "Validation_: [42/60] Losses: [243.644] Precision: [0.288] Recall: [0.288] Accuracy [0.288] f1-score: [0.288] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [43/60] Losses: [1086.380615] Time: 0.024 sec.\n",
      "Validation_: [43/60] Losses: [248.028] Precision: [0.292] Recall: [0.292] Accuracy [0.292] f1-score: [0.292] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [44/60] Losses: [1063.260498] Time: 0.029 sec.\n",
      "Validation_: [44/60] Losses: [251.556] Precision: [0.312] Recall: [0.312] Accuracy [0.312] f1-score: [0.312] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [45/60] Losses: [1055.504150] Time: 0.020 sec.\n",
      "Validation_: [45/60] Losses: [243.232] Precision: [0.312] Recall: [0.312] Accuracy [0.312] f1-score: [0.312] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [46/60] Losses: [1057.998047] Time: 0.025 sec.\n",
      "Validation_: [46/60] Losses: [256.281] Precision: [0.292] Recall: [0.292] Accuracy [0.292] f1-score: [0.292] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [47/60] Losses: [1028.657837] Time: 0.026 sec.\n",
      "Validation_: [47/60] Losses: [254.957] Precision: [0.302] Recall: [0.302] Accuracy [0.302] f1-score: [0.302] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [48/60] Losses: [995.443420] Time: 0.019 sec.\n",
      "Validation_: [48/60] Losses: [252.893] Precision: [0.298] Recall: [0.298] Accuracy [0.298] f1-score: [0.298] Time: 0.02 sec.\n",
      "  =------=  \n",
      "Train Epoch: [49/60] Losses: [986.002747] Time: 0.045 sec.\n",
      "Validation_: [49/60] Losses: [258.373] Precision: [0.305] Recall: [0.305] Accuracy [0.305] f1-score: [0.305] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [50/60] Losses: [930.084595] Time: 0.021 sec.\n",
      "Validation_: [50/60] Losses: [259.288] Precision: [0.285] Recall: [0.285] Accuracy [0.285] f1-score: [0.285] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [51/60] Losses: [916.824585] Time: 0.020 sec.\n",
      "Validation_: [51/60] Losses: [260.649] Precision: [0.292] Recall: [0.292] Accuracy [0.292] f1-score: [0.292] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [52/60] Losses: [881.616882] Time: 0.021 sec.\n",
      "Validation_: [52/60] Losses: [269.193] Precision: [0.288] Recall: [0.288] Accuracy [0.288] f1-score: [0.288] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [53/60] Losses: [862.031006] Time: 0.020 sec.\n",
      "Validation_: [53/60] Losses: [266.287] Precision: [0.275] Recall: [0.275] Accuracy [0.275] f1-score: [0.275] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [54/60] Losses: [810.635437] Time: 0.026 sec.\n",
      "Validation_: [54/60] Losses: [273.468] Precision: [0.285] Recall: [0.285] Accuracy [0.285] f1-score: [0.285] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [55/60] Losses: [792.738586] Time: 0.020 sec.\n",
      "Validation_: [55/60] Losses: [277.050] Precision: [0.288] Recall: [0.288] Accuracy [0.288] f1-score: [0.288] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [56/60] Losses: [738.529541] Time: 0.020 sec.\n",
      "Validation_: [56/60] Losses: [279.626] Precision: [0.292] Recall: [0.292] Accuracy [0.292] f1-score: [0.292] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [57/60] Losses: [715.782166] Time: 0.022 sec.\n",
      "Validation_: [57/60] Losses: [289.264] Precision: [0.285] Recall: [0.285] Accuracy [0.285] f1-score: [0.285] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [58/60] Losses: [674.276611] Time: 0.020 sec.\n",
      "Validation_: [58/60] Losses: [288.008] Precision: [0.285] Recall: [0.285] Accuracy [0.285] f1-score: [0.285] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [59/60] Losses: [673.575989] Time: 0.018 sec.\n",
      "Validation_: [59/60] Losses: [301.910] Precision: [0.302] Recall: [0.302] Accuracy [0.302] f1-score: [0.302] Time: 0.01 sec.\n",
      "  =------=  \n",
      "Train Epoch: [60/60] Losses: [693.832458] Time: 0.023 sec.\n",
      "Validation_: [60/60] Losses: [280.797] Precision: [0.315] Recall: [0.315] Accuracy [0.315] f1-score: [0.315] Time: 0.01 sec.\n",
      "  =------=  \n"
     ]
    }
   ],
   "source": [
    "path_save_embd = 'Flair/embeddings'\n",
    "\n",
    "TRAIN_EMBD = pickle.load(open(path_save_embd+'/train_embd.p', 'rb'))\n",
    "VAL_EMBD = pickle.load(open(path_save_embd+'/val_embd.p', 'rb'))\n",
    "TEST_EMBD = pickle.load(open(path_save_embd+'/test_embd.p', 'rb'))\n",
    "\n",
    "path_save_model1 = 'Flair/model1'\n",
    "path_save_model2 = 'Flair/model2'\n",
    "\n",
    "_check_dir(path_save_model1)\n",
    "_check_dir(path_save_model2)\n",
    "\n",
    "# hyper parameters\n",
    "epochs = 60\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# criterion = nn.MultiLabelMarginLoss()\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=learning_rate)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "\n",
    "data1_trn = [TRAIN_EMBD[0],TRAIN_EMBD[2]] # x,y, where x is the first embedding\n",
    "data2_trn = [TRAIN_EMBD[1],TRAIN_EMBD[2]] # x,y, where x is the second embedding\n",
    "\n",
    "data1_val = [VAL_EMBD[0],VAL_EMBD[2]] # x,y, where x is the first embedding\n",
    "data2_val = [VAL_EMBD[1],VAL_EMBD[2]] # x,y, where x is the first embedding\n",
    "\n",
    "# training and validation for model 1\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(model1, data1_trn, criterion, optimizer1, epoch, epochs)\n",
    "    test(model1, data1_val, criterion, epoch, epochs, 1)\n",
    "\n",
    "# training and validation for model 2\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(model2, data2_trn, criterion, optimizer2, epoch, epochs)\n",
    "    test(model2, data2_val, criterion, epoch, epochs, 1)\n",
    "\n",
    "# save current model\n",
    "# name_model = 'flair_1.pkl'\n",
    "# path_save_model = os.path.join(path_save_model1, name_model)\n",
    "# joblib.dump(model.float(), path_save_model, compress=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluting the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-1 Flair's DocumentPoolEmbeddings\n",
      "\n",
      "Validation_: [1/1] Losses: [651.811] Precision: [0.581] Recall: [0.581] Accuracy [0.581] f1-score: [0.581] Time: 0.14 sec.\n",
      "  =------=  \n",
      "model-2 Flair's DocumentLSTMEmbeddings\n",
      "\n",
      "Validation_: [1/1] Losses: [1283.009] Precision: [0.302] Recall: [0.302] Accuracy [0.302] f1-score: [0.302] Time: 0.04 sec.\n",
      "  =------=  \n",
      "model-3 Google's BERT embeddings\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-166-7b75522b59e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model-3 Google\\'s BERT embeddings\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata2_tst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model3' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "data1_tst = [TEST_EMBD[0],TEST_EMBD[2]] # x,y, where x is the first embedding\n",
    "data2_tst = [TEST_EMBD[1],TEST_EMBD[2]] # x,y, where x is the first embedding\n",
    "\n",
    "print('model-1 Flair\\'s DocumentPoolEmbeddings\\n')\n",
    "test(model1, data1_tst, criterion, 1, 1, 1)\n",
    "\n",
    "print('model-2 Flair\\'s DocumentLSTMEmbeddings\\n')\n",
    "test(model2, data2_tst, criterion, 1, 1, 1)\n",
    "\n",
    "print('model-3 Google\\'s BERT embeddings\\n')\n",
    "test(model3, data2_tst, criterion, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The final scores are as follows:\n",
    "\n",
    "| measure | Flair's LSTM | Flair' Pool | BERT \n",
    "|------|------|------|------|\n",
    "|  precision  | 0.920 | 0.961 | 0.961 |\n",
    "|  recall  | 0.925 | 0.958 | 0.958 |\n",
    "|  f1-score  | 0.922 | 0.960 | 0.960 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The results obtained are comparable with the state-of-the-art results presented in Sun et. al. (2018). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
